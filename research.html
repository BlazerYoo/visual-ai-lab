<div class="flex flex-col mt-12">

    <div class="flex flex-wrap justify-center">
        <div class="btn rounded-full badge border-2 badge-outline badge-warning font-bold research-filter mx-3 mb-2 text-xs sm:text-sm md:text-base" id="cv">computer vision, machine learning</div>
        <div class="btn rounded-full badge border-2 badge-outline badge-success font-bold research-filter mx-3 mb-2 text-xs sm:text-sm md:text-base" id="hci">human-computer interaction, cognitive science</div>
        <div class="btn rounded-full badge border-2 badge-outline badge-info font-bold research-filter mx-3 mb-2 text-xs sm:text-sm md:text-base" id="fair">fairness, accountability, and transparency</div>

    </div> 

    <div class="mt-10 text-right text-sm mx-10 md:mx-32 lg:mx-40">*our lab members pictured</div>
    
    <div class="mx-10 mt-5 md:mx-32 lg:mx-40">
                            <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2025        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Interactivity x Explainability: Toward Understanding How Interactivity Can Improve Computer Vision Explanations</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Indu Panigrahi, Sunnie S. Y. Kim*, Amna Liaqat*, Rohan Jinturkar, Olga Russakovsky, Ruth Fong and Parastoo Abtahi</p>
                          <p class="italic">(* = equal contribution)</p>
                        <p class="italic">ACM Conference on Human Factors in Computing Systems (CHI), Extended Abstract Track, 2025.</p>

            <p class="text-orange-800">
                          <a href="bib.html#panigrahi2025interactivity" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">transparency and explainability</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">D^3: Scaling Up Deepfake Detection by Learning from Discrepancy</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ye_zhu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/yu_wu.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Yongqi Yang, Zhihao Qian, Ye Zhu, Olga Russakovsky and Yu Wu</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2025.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2404.04584" class="hover:underline">[paper]</a>
                          <a href="bib.html#yang2025d3" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">deepfake detection</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">scaling</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Humans Attribute to Them</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/allison_chen.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/amaya_dharmasiri.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Allison Chen, Sunnie S. Y. Kim, Amaya Dharmasiri, Olga Russakovsky and Judith E. Fan</p>
                        <p class="italic">ACM Conference on Human Factors in Computing Systems (CHI), Extended Abstract Track, 2025.</p>

            <p class="text-orange-800">
                          <a href="bib.html#chen2025portraying" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">psychology</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">mental capacity attributions</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo and Olga Russakovsky</p>
                        <p class="italic">ACM Conference on Human Factors in Computing Systems (CHI), 2025.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2502.08554v1" class="hover:underline">[paper]</a>
                          <a href="bib.html#kim2025fostering" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">trust and reliance</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Unifying Specialized Visual Encoders for Video Language Models</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/jihoon_chung.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/tyler_zhu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/max_gonzalez_saez-diez.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Jihoon Chung*, Tyler Zhu*, Max Gonzalez Saez-Diez, Juan Carlos Niebles, Honglu Zhou and Olga Russakovsky</p>
                        <p class="italic">arXiv preprint arXiv:2501.01426, 2025.</p>

            <p class="text-orange-800">
                          <a href="https://www.arxiv.org/abs/2501.01426" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/merv" class="hover:underline">[code]</a>
                          <a href="https://tylerzhu.com/merv" class="hover:underline">[website]</a>
                          <a href="bib.html#chung2025unifying" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">video</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">video understanding</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">multimodal LLMs</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2024        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ICONS: Influence Consensus for Vision-Language Data Selection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/xindi_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Xindi Wu, Mengzhou Xia, Rulin Shao, Zhiwei Deng, Pang Wei Koh and Olga Russakovsky</p>
                        <p class="italic">arXiv preprint arXiv:2501.00654, 2024.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2501.00654" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/icons" class="hover:underline">[code]</a>
                          <a href="https://princetonvisualai.github.io/icons/" class="hover:underline">[website]</a>
                          <a href="bib.html#wu2024icons" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">vision and language</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data selection</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data for efficient learning</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/xindi_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Xindi Wu*, Dingli Yu*, Yangsibo Huang*, Olga Russakovsky and Sanjeev Arora</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2024.</p>

            <p class="text-orange-800">
                          <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9c3563bbeb2ad7f3b3b8ed0fcd3b440f-Paper-Datasets_and_Benchmarks_Track.pdf" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/ConceptMix" class="hover:underline">[code]</a>
                          <a href="https://princetonvisualai.github.io/conceptmix/" class="hover:underline">[website]</a>
                          <a href="bib.html#wu2024conceptmix" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">compositionality</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">image generation</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">evaluation</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6   fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Benchmark Suites Instead of Leaderboards for Evaluating AI Fairness</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Angelina Wang, Aaron Hertzmann and Olga Russakovsky</p>
                        <p class="italic">Patterns, 2024.</p>

            <p class="text-orange-800">
                          <a href="https://www.cell.com/patterns/fulltext/S2666-3899%2824%2900239-3" class="hover:underline">[paper]</a>
                          <a href="bib.html#wang2024benchmark" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">evaluation</div>
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">AI fairness</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">What is Dataset Distillation Learning?</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/william_yang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ye_zhu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>William Yang, Ye Zhu, Zhiwei Deng and Olga Russakovsky</p>
                        <p class="italic">International Conference on Machine Learning (ICML), 2024.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2406.04284" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/What-is-Dataset-Distillation-Learning/" class="hover:underline">[code]</a>
                          <a href="bib.html#yang2024what" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">dataset distillation</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Analyzing the Roles of Language and Vision in Learning from Limited Data</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/allison_chen.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Allison Chen, Ilia Sucholutsky, Olga Russakovsky and Tom Griffiths</p>
                        <p class="italic">Proceedings of the Annual Meeting of the Cognitive Science Society (CogSci), 2024.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2403.19669" class="hover:underline">[paper]</a>
                          <a href="bib.html#chen2024analyzing" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">vision and language</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">cognitive science</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">cognitive architecture</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/william_yang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/byron_zhang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>William Yang, Byron Zhang and Olga Russakovsky</p>
                        <p class="italic">International Conference on Learning Representations (ICLR), 2024.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2310.01755" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/imagenetood" class="hover:underline">[code]</a>
                          <a href="bib.html#yang2024imagenetood" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">OOD</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">analysis</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">dataset</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Vision-Language Dataset Distillation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/xindi_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/byron_zhang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Xindi Wu, Byron Zhang, Zhiwei Deng and Olga Russakovsky</p>
                        <p class="italic">Transactions on Machine Learning Research (TMLR), 2024.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2308.07545" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/multimodal_dataset_distillation" class="hover:underline">[code]</a>
                          <a href="bib.html#wu2024visionlanguage" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">vision and language</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">dataset distillation</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data for efficient learning</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2023        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nobline_yoo.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Nobline Yoo and Olga Russakovsky</p>
                        <p class="italic">International Conference on Computer Vision (ICCVW) ROAD&#43;&#43; Workshop, 2023.</p>

            <p class="text-orange-800">
                          <a href="https://openaccess.thecvf.com/content/ICCV2023W/ROAD&#43;&#43;/papers/Yoo_Efficient_Self-Supervised_Human_Pose_Estimation_with_Inductive_Prior_Tuning_ICCVW_2023_paper.pdf" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/hpe-inductive-prior-tuning" class="hover:underline">[code]</a>
                          <a href="bib.html#yoo2023efficient" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">human pose estimation</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">self-supervised</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Boundary Guided Learning-Free Semantic Control with Diffusion Models</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ye_zhu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/yu_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky and Yan Yan</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2302.08357" class="hover:underline">[paper]</a>
                          <a href="https://github.com/L-YeZhu/BoundaryDiffusion" class="hover:underline">[code]</a>
                          <a href="https://l-yezhu.github.io/BoundaryDiffusion/" class="hover:underline">[website]</a>
                          <a href="bib.html#zhu2023boundary" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">diffusion models</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">image generation</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">controllable generation</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv  fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sing_yu_lin.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/dora_zhao.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron B. Adcock, Laurens van der Maaten, Deepti Ghadiyaram and Olga Russakovsky</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2301.02560" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/geode_dataset" class="hover:underline">[code]</a>
                          <a href="https://geodiverse-data-collection.cs.princeton.edu/" class="hover:underline">[website]</a>
                          <a href="bib.html#ramaswamy2023geode" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">new benchmark</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">object recognition</div>
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">geodiversity</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv  fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Overwriting Pretrained Bias with Finetuning Data</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Angelina Wang and Olga Russakovsky</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2303.06167" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/overcoming-pretraining-bias" class="hover:underline">[code]</a>
                          <a href="bib.html#wang2023overwriting" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">algorithmic intervention</div>
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">AI fairness</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv  fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Gender Artifacts in Visual Datasets</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nicole_meister.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/dora_zhao.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Nicole Meister*, Dora Zhao*, Angelina Wang, Vikram V. Ramaswamy, Ruth Fong and Olga Russakovsky</p>
                          <p class="italic">(* = equal contribution)</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2206.09191" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/gender-artifacts" class="hover:underline">[code]</a>
                          <a href="bib.html#meister2023gender" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data analysis</div>
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">AI fairness</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6   fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Art and the Science of Generative AI</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Ziv Epstein, Aaron Hertzmann, the Investigators of Human Creativity (Memo Akten, Hany Farid, Jessica Fjeld, Morgan R. Frank, Matthew Groh, Laura Herman, Neil Leach, Robert Mahari, Alex Pentland, Olga Russakovsky, Hope Schroeder and Amy Smith)</p>
                        <p class="italic">Science Perspectives, 2023.</p>

            <p class="text-orange-800">
                          <a href="https://www.science.org/doi/10.1126/science.adh4451" class="hover:underline">[paper]</a>
                          <a href="https://arxiv.org/abs/2306.04141" class="hover:underline">[extended white paper]</a>
                          <a href="bib.html#epstein2023art" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">AI and society</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Discrete Diffusion Reward Guidance Methods for Offline Reinforcement Learning</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/matthew_coleman.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ye_zhu.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Matthew Coleman, Olga Russakovsky, Christine Allen-Blanchette and Ye Zhu</p>
                        <p class="italic">International Conference on Machine Learning (ICMLW) Sampling and Optimization in Discrete Space Workshop, 2023.</p>

            <p class="text-orange-800">
                          <a href="https://openreview.net/pdf?id=s4cSgzGudq" class="hover:underline">[paper]</a>
                          <a href="bib.html#coleman2023discrete" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">offline RL</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">diffusion policy</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv  fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ICON^2: Reliably Benchmarking Predictive Inequity in Object Detection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Sruthi Sudhakar, Viraj Prabhu, Olga Russakovsky and Judy Hoffman</p>
                        <p class="italic">arXiv preprint arXiv:2306.04482, 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2306.04482" class="hover:underline">[paper]</a>
                          <a href="bib.html#sudhakar2023icon2" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">object detection</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">evaluation</div>
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">fairness benchmarking</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Humans, AI, and Context: Understanding End-Users&#39; Trust in a Real-World Computer Vision Application</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Sunnie S. Y. Kim, Elizabeth Anne Watkins, Olga Russakovsky, Ruth Fong and Andres Monroy-Hernandez</p>
                        <p class="italic">ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2305.08598" class="hover:underline">[paper]</a>
                          <a href="bib.html#kim2023humans" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">trust and reliance</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Overlooked Factors in Concept-based Explanations: Dataset Choice, Concept Learnability, and Human Capability</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Vikram V. Ramaswamy, Sunnie S. Y. Kim, Ruth Fong and Olga Russakovsky</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2207.09615" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/OverlookedFactors" class="hover:underline">[code]</a>
                          <a href="bib.html#ramaswamy2023overlooked" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">faithfulness</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">explainability</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">&#34;Help Me Help the AI&#34;: Understanding How Explainability Can Support Human-AI Interaction</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Sunnie S. Y. Kim, Elizabeth Anne Watkins, Olga Russakovsky, Ruth Fong and Andres Monroy-Hernandez</p>
                        <p class="italic">ACM Conference on Human Factors in Computing Systems (CHI), 2023.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2210.03735" class="hover:underline">[paper]</a>
                          <a href="https://github.com/sunniesuhyoung/publicfiles/blob/main/Kim2023HelpMeHelpTheAI_supp.pdf" class="hover:underline">[supplement]</a>
                          <a href="https://youtu.be/PD8a7aEQPf4" class="hover:underline">[30-sec video]</a>
                          <a href="https://youtu.be/IcVsi5-ON4" class="hover:underline">[10-min video]</a>
                          <a href="bib.html#kim2023help" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">transparency and explainability</div>
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2022        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Siri: A simple selective retraining mechanism for transformer-based visual grounding</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/yu_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Mengxue Qu, Yu Wu, Wu Liu, Qiqi Gong, Xiaodan Liang, Olga Russakovsky, Yao Zhao and Yunchao Wei</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/pdf/2207.13325" class="hover:underline">[paper]</a>
                          <a href="bib.html#qu2022siri" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">Visual grounding</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">Transformer</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">Generalization</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ELUDE: Generating Interpretable Explanations via a Decomposition into Labelled and Unlabelled Features</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nicole_meister.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Vikram V. Ramaswamy, Sunnie S. Y. Kim, Nicole Meister, Ruth Fong and Olga Russakovsky</p>
                        <p class="italic">arXiv preprint arXiv:2206.07690, 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2206.07690" class="hover:underline">[paper]</a>
                          <a href="bib.html#ramaswamy2022elude" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Learning Actionness from Action/Background Discrimination</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ozge_yalcinkaya_simsek.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Ozge Yalcinkaya Simsek, Olga Russakovsky and Pinar Duygulu</p>
                        <p class="italic">Signal, Image and Video Processing (SIViP), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://link.springer.com/article/10.1007/s11760-022-02369-y" class="hover:underline">[paper]</a>
                          <a href="bib.html#simsek2022learning" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Enabling Detailed Action Recognition Evaluation Through Video Dataset Augmentation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/jihoon_chung.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/yu_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Jihoon Chung, Yu Wu and Olga Russakovsky</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2022.</p>

            <p class="text-orange-800">
                          <a href="https://openreview.net/forum?id=eOnQ2etkxto" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/HAT" class="hover:underline">[code]</a>
                          <a href="bib.html#chung2022enabling" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Zhiwei Deng and Olga Russakovsky</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2206.02916" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/RememberThePast-DatasetDistillation" class="hover:underline">[code]</a>
                          <a href="bib.html#deng2022remember" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">HIVE: Evaluating the Human Interpretability of Visual Explanations</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nicole_meister.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ruth_fong.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Sunnie S. Y. Kim, Nicole Meister, Vikram V. Ramaswamy, Ruth Fong and Olga Russakovsky</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2112.03184" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/HIVE" class="hover:underline">[code]</a>
                          <a href="https://princetonvisualai.github.io/HIVE/" class="hover:underline">[website]</a>
                          <a href="https://drive.google.com/file/d/1nOYfy_0e61cGGDzwreCI4IUly1VbTgur/view?usp=sharing" class="hover:underline">[extended abstract]</a>
                          <a href="https://youtu.be/BDlFb1CFQRQ" class="hover:underline">[2-min video]</a>
                          <a href="https://youtu.be/7uysq-qAtr4" class="hover:underline">[8-min video]</a>
                          <a href="bib.html#kim2022hive" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human-AI interaction</div>
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">transparency and explainability</div>
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Multi-Query Video Retrieval</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zeyu_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/yu_wu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Zeyu Wang, Yu Wu, Karthik Narasimhan and Olga Russakovsky</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2201.03639" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/MQVR" class="hover:underline">[code]</a>
                          <a href="bib.html#wang2022multiquery" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresentation, and Performing Evaluation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Angelina Wang, Vikram V. Ramaswamy and Olga Russakovsky</p>
                        <p class="italic">ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2205.04610" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/intersectionality" class="hover:underline">[code]</a>
                          <a href="bib.html#wang2022towards" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">CARETS: A Consistency And Robustness Evaluative Test Suite for VQA</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Carlos E. Jimenez, Olga Russakovsky and Karthik Narasimhan</p>
                        <p class="italic">Association for Computational Linguistics (ACL), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2203.07613" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princeton-nlp/CARETS" class="hover:underline">[code]</a>
                          <a href="bib.html#jimenez2022carets" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">A Study of Face Obfuscation in ImageNet</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng and Olga Russakovsky</p>
                        <p class="italic">International Conference on Machine Learning (ICML), 2022.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2103.06191" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/imagenet-face-obfuscation" class="hover:underline">[code]</a>
                          <a href="http://image-net.org/face-obfuscation/" class="hover:underline">[project]</a>
                          <a href="bib.html#yang2022a" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2021        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Understanding and Evaluating Racial Biases in Image Captioning</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/dora_zhao.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Dora Zhao, Angelina Wang and Olga Russakovsky</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2021.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2106.08503" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/imagecaptioning-bias" class="hover:underline">[code]</a>
                          <a href="https://princetonvisualai.github.io/imagecaptioning-bias/" class="hover:underline">[website]</a>
                          <a href="bib.html#zhao2021understanding" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Point and Ask: Incorporating Pointing into Visual Question Answering</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/arjun_mani.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nobline_yoo.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/will_hinthorn.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Arjun Mani, Nobline Yoo, Will Hinthorn and Olga Russakovsky</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPRW) Visual Question Answering Workshop, 2021.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2011.13681" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/pointingqa" class="hover:underline">[code]</a>
                          <a href="https://hinthornw.github.io/pointingqa/" class="hover:underline">[website]</a>
                          <a href="bib.html#mani2021point" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">VQA</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data</div>
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">pointing</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">human supervision</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Directional Bias Amplification</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Angelina Wang and Olga Russakovsky</p>
                        <p class="italic">International Conference on Machine Learning (ICML), 2021.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2102.12594" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/directional-bias-amp" class="hover:underline">[code]</a>
                          <a href="bib.html#wang2021directional" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">[Re] Don&#39;t Judge an Object by Its Context: Learning to Overcome Contextual Bias</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sharon_zhang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/nicole_meister.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Sunnie S. Y. Kim, Sharon Zhang, Nicole Meister and Olga Russakovsky</p>
                          <p class="italic">ML Reproducibility Challenge, 2020.</p>
                        <p class="italic">ReScience C, 2021.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2104.13582" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/ContextualBias" class="hover:underline">[code]</a>
                          <a href="bib.html#kim2021re" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Fair Attribute Classification through Latent Space De-biasing</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/vikram_v_ramaswamy.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/sunnie_s_y_kim.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Vikram V. Ramaswamy, Sunnie S. Y. Kim and Olga Russakovsky</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2021.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2012.01469" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/gan-debiasing" class="hover:underline">[code]</a>
                          <a href="https://princetonvisualai.github.io/gan-debiasing/" class="hover:underline">[website]</a>
                          <a href="bib.html#ramaswamy2021fair" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2020        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Zhiwei Deng, Karthik Narasimhan and Olga Russakovsky</p>
                        <p class="italic">Neural Information Processing Systems (NeurIPS), 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2007.05655" class="hover:underline">[paper]</a>
                          <a href="bib.html#deng2020evolving" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">CornerNet-Lite: Efficient Keypoint Based Object Detection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Hei Law, Yun Teng, Olga Russakovsky and Jia Deng</p>
                        <p class="italic">British Machine Vision Conference (BMVC), 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1904.08900" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princeton-vl/CornerNet-Lite" class="hover:underline">[code]</a>
                          <a href="bib.html#law2020cornernetlite" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/angelina_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Angelina Wang, Arvind Narayanan and Olga Russakovsky</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2004.07999" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/vibe-tool" class="hover:underline">[code]</a>
                          <a href="https://www.youtube.com/watch?v=khUlfk7moDI&amp;list=PLlXRttA01GyEi0lyTdcRf3R3ILmYv2qo-&amp;index=10&amp;t=0s" class="hover:underline">[video]</a>
                          <a href="https://youtu.be/4ouwCgCOyrs" class="hover:underline">[90-sec video]</a>
                          <a href="https://youtu.be/PkbXhM5BlSM" class="hover:underline">[10-min video]</a>
                          <a href="bib.html#wang2020revise" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Towards Unique and Informative Captioning of Images</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zeyu_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/berthy_feng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Zeyu Wang, Berthy Feng, Karthik Narasimhan and Olga Russakovsky</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2020.</p>

            <p class="text-orange-800">
                          <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/html/350_ECCV_2020_paper.php" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/SPICE-U" class="hover:underline">[code]</a>
                          <a href="http://visualai.princeton.edu/posters/wangECCV2020-1minVideo.mp4" class="hover:underline">[1-min video]</a>
                          <a href="http://visualai.princeton.edu/posters/wangECCV2020-10minVideo.mp4" class="hover:underline">[10-min video]</a>
                          <a href="bib.html#wang2020towards" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Take The Scenic Route: Improving Generalization In Vision-and-language Navigation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/felix_yu.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zhiwei_deng.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Felix Yu, Zhiwei Deng, Karthik Narasimhan and Olga Russakovsky</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPRW) Visual Learning with Limited Labels Workshop, 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/2003.14269" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/VLNActionPriors" class="hover:underline">[code]</a>
                          <a href="https://www.youtube.com/watch?v=ItA4NpPaO70&amp;feature=youtu.be" class="hover:underline">[video]</a>
                          <a href="bib.html#yu2020take" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Towards Fairness In Visual Recognition: Effective Strategies For Bias Mitigation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/zeyu_wang.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/ioannis_c_karakozis.jpg">
                        </div>
                    </div>
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Zeyu Wang, Klint Qinami, Ioannis C. Karakozis, Kyle Genova, Prem Nair, Kenji Hata and Olga Russakovsky</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1911.11834" class="hover:underline">[paper]</a>
                          <a href="https://github.com/princetonvisualai/DomainBiasMitigation" class="hover:underline">[code]</a>
                          <a href="http://visualai.princeton.edu/posters/wang2020fair-1min.mp4" class="hover:underline">[1-min video]</a>
                          <a href="bib.html#wang2020towards" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6    paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Towards Fairer Datasets: Filtering And Balancing The Distribution Of The People Subtree In The Imagenet Hierarchy</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng and Olga Russakovsky</p>
                        <p class="italic">Conference on Fairness, Accountability and Transparency (FAT*), 2020.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1912.07726" class="hover:underline">[paper]</a>
                          <a href="http://image-net.org/filtering-and-balancing/" class="hover:underline">[project]</a>
                          <a href="https://www.wired.com/story/ai-biased-how-scientists-trying-fix/" class="hover:underline">[Wired article]</a>
                          <a href="bib.html#yang2020towards" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2019        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Kaiyu Yang, Olga Russakovsky and Jia Deng</p>
                        <p class="italic">Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1908.02660" class="hover:underline">[paper]</a>
                          <a href="bib.html#yang2019spatialsense" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">spatial relationship recognition</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">benchmarking</div>
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">adversarial crowdsourcing</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Compositional Temporal Visual Grounding of Natural Language Event Descriptions</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Jonathan Stroud, Ryan McCaffrey, Rada Mihalcea, Jia Deng and Olga Russakovsky</p>
                        <p class="italic">arxiv preprint arXiv:1912.02256, 2019.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1912.02256" class="hover:underline">[paper]</a>
                          <a href="https://www.jonathancstroud.com/ctg.html" class="hover:underline">[project]</a>
                          <a href="bib.html#stroud2019compositional" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">video understanding</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">temporal grounding</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Human Uncertainty Makes Classification More Robust</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Joshua C. Peterson*, Ruairidh M. Battleday*, Thomas L. Griffiths and Olga Russakovsky</p>
                          <p class="italic">(* = equal contribution)</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2019.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1908.07086" class="hover:underline">[paper]</a>
                          <a href="bib.html#peterson2019human" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">uncertainty</div>
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">cognition</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">An Adversarially Crowdsourced Benchmark For Spatial Relation Recognition</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Kaiyu Yang, Olga Russakovsky and Jia Deng</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2019.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1908.02660" class="hover:underline">[paper]</a>
                          <a href="bib.html#yang2019an" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2018        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">The more you look, the more you see: towards general object understanding through recursive refinement</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Jingyan Wang, Olga Russakovsky and Deva Ramanan</p>
                        <p class="italic">Winter Conference on Applications of Computer Vision (WACV), 2018.</p>

            <p class="text-orange-800">
                          <a href="https://www.cs.cmu.edu/~jingyanw/papers/refinement.pdf" class="hover:underline">[paper]</a>
                          <a href="https://github.com/jingyanw/recursive-refinement" class="hover:underline">[code]</a>
                          <a href="https://www.cs.cmu.edu/~jingyanw/papers/refinement_supp.pdf" class="hover:underline">[supplement]</a>
                          <a href="bib.html#wang2018the" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">object understanding</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2017        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">What Actions Are Needed For Understanding Human Actions In Videos?</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Gunnar Sigurdsson, Olga Russakovsky and Abhinav Gupta</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2017.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1708.02696" class="hover:underline">[paper]</a>
                          <a href="bib.html#sigurdsson2017what" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">human action recognition</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori and Li Fei-Fei</p>
                        <p class="italic">International Journal of Computer Vision (IJCV), 2017.</p>

            <p class="text-orange-800">
                          <a href="http://arxiv.org/abs/1507.05738" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~syyeung/everymoment.html" class="hover:underline">[project]</a>
                          <a href="bib.html#yeung2017every" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">data annotation</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">What&#39;s in a Question: Using Visual Questions as a Form of Supervision</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Siddha Ganju, Olga Russakovsky and Abhinav Gupta</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2017.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1704.03895" class="hover:underline">[paper]</a>
                          <a href="http://sidgan.me/whats_in_a_question/" class="hover:underline">[project]</a>
                          <a href="bib.html#ganju2017whats" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">VQA</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Predictive-Corrective Networks for Action Detection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Achal Dave, Olga Russakovsky and Deva Ramanan</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2017.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1704.03615" class="hover:underline">[paper]</a>
                          <a href="http://www.achaldave.com/projects/predictive-corrective/" class="hover:underline">[project]</a>
                          <a href="bib.html#dave2017predictivecorrective" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">action detection</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Learning to Learn from Noisy Web Videos</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori and Li Fei-Fei</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2017.</p>

            <p class="text-orange-800">
                          <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yeung_Learning_to_Learn_CVPR_2017_paper.pdf" class="hover:underline">[paper]</a>
                          <a href="http://openaccess.thecvf.com/content_cvpr_2017/poster/2140_POSTER.pdf" class="hover:underline">[poster]</a>
                          <a href="bib.html#yeung2017learning" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">video</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">human action recognition</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2016        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Crowdsourcing in Computer Vision</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Adriana Kovashka, Olga Russakovsky, Li Fei-Fei and Kristen Grauman</p>
                        <p class="italic">Foundation and Trends in Computer Vision and Graphics, 2016.</p>

            <p class="text-orange-800">
                          <a href="https://arxiv.org/abs/1611.02145" class="hover:underline">[paper]</a>
                          <a href="bib.html#kovashka2016crowdsourcing" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">data annotation</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Much Ado About Time: Exhaustive Annotation of Temporal Data</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Gunnar A. Sigurdsson, Olga Russakovsky, Ali Farhadi, Ivan Laptev and Abhinav Gupta</p>
                        <p class="italic">Conference on Human Computation and Crowdsourcing (HCOMP), 2016.</p>

            <p class="text-orange-800">
                          <a href="http://arxiv.org/abs/1607.07429" class="hover:underline">[paper]</a>
                          <a href="http://allenai.org/plato/charades/" class="hover:underline">[project]</a>
                          <a href="https://visualai.princeton.edu/posters/SigRusFarLapGup16.pdf" class="hover:underline">[poster]</a>
                          <a href="https://visualai.princeton.edu/slides/much_ado_about_time_1nov2016.key" class="hover:underline">[slides key]</a>
                          <a href="https://visualai.princeton.edu/slides/much_ado_about_time_1nov2016.pdf" class="hover:underline">[slides pdf]</a>
                          <a href="bib.html#sigurdsson2016much" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">data annotation</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">What&#39;s the Point: Semantic Segmentation with Point Supervision</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Amy Bearman, Olga Russakovsky, Vittorio Ferrari and Li Fei-Fei</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2016.</p>

            <p class="text-orange-800">
                          <a href="http://arxiv.org/abs/1506.02106" class="hover:underline">[paper]</a>
                          <a href="http://vision.stanford.edu/whats_the_point" class="hover:underline">[project]</a>
                          <a href="bib.html#bearman2016whats" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">semantic segmentation</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">End-to-end Learning of Action Detection from Frame Glimpses in Videos</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Serena Yeung, Olga Russakovsky, Greg Mori and Li Fei-Fei</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2016.</p>

            <p class="text-orange-800">
                          <a href="http://arxiv.org/abs/1511.06984" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~syyeung/frameglimpses.html" class="hover:underline">[project]</a>
                          <a href="bib.html#yeung2016endtoend" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">video</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">human action recognition</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6   fair paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Towards More Gender Diversity in CS through an Artificial Intelligence Summer Program for High School Girls</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Marie E. Vachovsky, Grace Wu, Sorathan Chaturapruek, Olga Russakovsky, Rick Sommer and Li Fei-Fei</p>
                        <p class="italic">Special Interest Group on Computer Science Education (SIGCSE), 2016.</p>

            <p class="text-orange-800">
                          <a href="http://stanford.edu/~sorathan/papers/SAILORS-SIGCSE2016.pdf" class="hover:underline">[paper]</a>
                          <a href="http://sailors.stanford.edu/" class="hover:underline">[SAILORS camp homepage]</a>
                          <a href="http://www.wired.com/2015/08/stanford-artificial-intelligence-laboratorys-outreach-summer-program/" class="hover:underline">[Wired article]</a>
                          <a href="bib.html#vachovsky2016towards" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
              
                              <div class="badge badge-outline badge-info mr-1 h-auto text-xs sm:text-sm md:text-base" id="fair">outreach</div>
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2015        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Scaling up Object Detection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                  <div class="avatar size-0 sm:size-8 ml-2"> 
                        <div class="rounded-full">
                            <img src="assets/olga_russakovsky.jpg">
                        </div>
                    </div>
                                </div>
            </div>
            <p>Olga Russakovsky</p>
                        <p class="italic">PhD Thesis, Stanford University, 2015.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/PhD_thesis.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/posters/doctoral_consortium_poster.pdf" class="hover:underline">[poster]</a>
                          <a href="bib.html#russakovsky2015scaling" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">object detection</div>
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">PhD thesis</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Best of both worlds: human-machine collaboration for object annotation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky, Li-Jia Li and Li Fei-Fei</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2015.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/RussakovskyCVPR15.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/best_of_both_worlds.html" class="hover:underline">[project]</a>
                          <a href="bib.html#russakovsky2015best" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">human-in-the-loop</div>
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">data annotation</div>
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Joint calibration of Ensemble of Exemplar SVMs</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Davide Modolo, Alexander Vezhnevets, Olga Russakovsky and Vittorio Ferrari</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2015.</p>

            <p class="text-orange-800">
                          <a href="http://arxiv.org/abs/1503.00783" class="hover:underline">[paper]</a>
                          <a href="http://calvin.inf.ed.ac.uk/software/jointcalibration/" class="hover:underline">[code]</a>
                          <a href="bib.html#modolo2015joint" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">SVMs</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">ImageNet Large Scale Visual Recognition Challenge</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander Berg and Li Fei-Fei</p>
                          <p class="italic">(* = equal contribution)</p>
                        <p class="italic">International Journal of Computer Vision (IJCV), 2015.</p>

            <p class="text-orange-800">
                          <a href="http://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign=email/event/articleAuthor/onlineFirst#" class="hover:underline">[paper]</a>
                          <a href="http://arxiv.org/abs/1409.0575" class="hover:underline">[paper content on arxiv]</a>
                          <a href="http://image-net.org/challenges/LSVRC/2014/ui/det.html" class="hover:underline">[browse detection data]</a>
                          <a href="http://ai.stanford.edu/~olga/data/ILSVRC_annotations_ICCV13.zip" class="hover:underline">[attribute annotations]</a>
                          <a href="http://image-net.org/challenges/LSVRC" class="hover:underline">[ILSVRC homepage]</a>
                          <a href="bib.html#russakovsky2015imagenet" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data collection</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2014        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6  hci  paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Scalable Multi-Label Annotation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Jia Deng, Olga Russakovsky, Jonathan Krause, Michael Bernstein, Alexander Berg and Li Fei-Fei</p>
                        <p class="italic">ACM Conference on Human Factors in Computing Systems (CHI), 2014.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/chi2014-MultiLabel.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~jkrause/papers/chi14_pres.pdf" class="hover:underline">[slides]</a>
                          <a href="bib.html#deng2014scalable" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
              
                              <div class="badge badge-outline badge-success mr-1 h-auto text-xs sm:text-sm md:text-base" id="hci">data annotation</div>
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2013        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Detecting avocados to zucchinis: what have we done, and where are we going?</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander Berg and Li Fei-Fei</p>
                        <p class="italic">International Conference on Computer Vision (ICCV), 2013.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/iccv13-ILSVRCanalysis.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/papers/iccv13-ILSVRCanalysis-supp.pdf" class="hover:underline">[supplement]</a>
                          <a href="http://image-net.org/challenges/LSVRC/2012/analysis/" class="hover:underline">[additional analysis]</a>
                          <a href="http://ai.stanford.edu/~olga/data/ILSVRC_annotations_ICCV13.zip" class="hover:underline">[attribute annotations]</a>
                          <a href="http://ai.stanford.edu/~olga/posters/iccv13_poster.pdf" class="hover:underline">[poster]</a>
                          <a href="http://www.youtube.com/watch?v=DK6KfUsVN8w&amp;list=PLb0IAmt7-GS24xHBtQc-u3A0u32kn2kIi&amp;index=4" class="hover:underline">[poster of talk at BAVM]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/ImageNetAnalysis_bavm_10_5_13.pptx" class="hover:underline">[slides pptx]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/ImageNetAnalysis_bavm_10_5_13.pdf" class="hover:underline">[slides pdf]</a>
                          <a href="bib.html#russakovsky2013detecting" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">data analysis</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2012        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Object-centric spatial pooling for image classification</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky, Yuanqing Lin, Kai Yu and Li Fei-Fei</p>
                        <p class="italic">European Conference on Computer Vision (ECCV), 2012.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/eccv12-OCP.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/posters/eccv12-poster.pdf" class="hover:underline">[poster]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/OlgaRussakovsky_OCPtalk_032513.pptx" class="hover:underline">[slides pptx]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/OlgaRussakovsky_OCPtalk_032513.pdf" class="hover:underline">[slides pdf]</a>
                          <a href="http://ai.stanford.edu/~olga/videos/eccv12-video.mov" class="hover:underline">[30-sec spotlight]</a>
                          <a href="http://ai.stanford.edu/~olga/OCP_FAQ.html" class="hover:underline">[FAQ]</a>
                          <a href="bib.html#russakovsky2012objectcentric" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">image classification</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2010        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Attribute learning in large-scale data</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky and Li Fei-Fei</p>
                        <p class="italic">European Conference on Computer Vision (ECCVW) Parts and Attributes Workshop, 2010.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/eccv10workshop-Attributes.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/eccv10attributes-largescale.odp" class="hover:underline">[slides odp]</a>
                          <a href="http://ai.stanford.edu/~olga/slides/eccv10attributes-largescale.pdf" class="hover:underline">[slides pdf]</a>
                          <a href="http://www.image-net.org/download-attributes" class="hover:underline">[data]</a>
                          <a href="bib.html#russakovsky2010attribute" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">image classification</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">A Steiner tree approach to efficient object detection</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Olga Russakovsky and Andrew Y. Ng</p>
                        <p class="italic">Computer Vision and Pattern Recognition (CVPR), 2010.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/cvpr10-SteinerTrees.pdf" class="hover:underline">[paper]</a>
                          <a href="http://ai.stanford.edu/~olga/posters/cvpr10-poster.pdf" class="hover:underline">[poster]</a>
                          <a href="http://ai.stanford.edu/~olga/data/office_scenes.zip" class="hover:underline">[data]</a>
                          <a href="bib.html#russakovsky2010a" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">object detection</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Autonomous operation of novel elevators for robot navigation</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Ellen Klingbeil, Blake Carpenter, Olga Russakovsky and Andrew Y. Ng</p>
                        <p class="italic">International Conference on Robotics and Automation (ICRA), 2010.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/icra10-OperationOfNovelElevators.pdf" class="hover:underline">[paper]</a>
                          <a href="bib.html#klingbeil2010autonomous" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">robotics</div>
              
              
                          </div>
          </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">STanford AI Robot (STAIR) Vision Library</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Stephen Gould, Olga Russakovsky, Ian Goodfellow, Paul Baumstarck, Andrew Y. Ng and Daphne Koller</p>
                        <p class="italic">http://ai.stanford.edu/~sgould/svl, 2010.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~sgould/svl/" class="hover:underline">[code]</a>
                          <a href="http://stair.stanford.edu/" class="hover:underline">[project]</a>
                          <a href="bib.html#gould2010stanford" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">robotics</div>
              
              
                          </div>
          </div>
                              <div class="border-l-8 pl-2 text-orange-400 mt-5 font-mono font-bold text-base sm:text-lg">
            2007        </div>
                            
          <div class="m-3 sm:mx-5 text-xs sm:text-sm md:text-base mb-6 cv   paper">
            <div class="flex flex-row">
                <h1 class="self-end font-bold">Training Conditional Random Fields for maximum labelwise accuracy</h1> 

                
                <div class="flex flex-row invisible sm:visible">
                                </div>
            </div>
            <p>Samuel S. Gross, Olga Russakovsky, Chuong B. Do and Serafim Batzoglou</p>
                        <p class="italic">Advances in Neural Information Processing Systems (NeurIPS), 2007.</p>

            <p class="text-orange-800">
                          <a href="http://ai.stanford.edu/~olga/papers/nips2006-CRFtraining.pdf" class="hover:underline">[paper]</a>
                          <a href="bib.html#gross2007training" class="hover:underline">[bibtex]</a>
                        </p>

            
            <div class="flex flex-wrap">
                              <div class="badge badge-outline badge-warning mr-1 h-auto text-xs sm:text-sm md:text-base" id="cv">labelwise accuracy</div>
              
              
                          </div>
          </div>
              
    </div>
</div>

<div class="h-40"></div>